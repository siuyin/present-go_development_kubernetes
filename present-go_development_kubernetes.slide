Go development for Kubernetes
From code to deployment
9 Jan 2020

Loh Siu Yin
Technology Consultant, Beyond Broadcast LLP
siuyin@beyondbroadcast.com



* From code to deployment

- Write code: Go 
- Test: unit tests
- Containerize: Dockerfile
- Deploy (Test [functional, integration, end-to-end]: kustomize, skaffold
- Deploy: Staging, Production, Metrics, Logging




* Writing Go code

First outline what you want to build, define system level components.

  Go Hello World web app
  NATS Streaming provides internal messaging and stores state
  
  Deployed on Kubernetes

Then define Go app's major modules.

  Web request routing: main.go
  Web request handlers: / -> time service (time.go)



* Set up go development environment:

.link https://golang.org/dl/

  > go version
  go version go1.13.4 linux/amd64



* Initialise go module

  > go mod init github.com/siuyin/present-go_development_kubernetes
  go: creating new go.mod: module github.com/siuyin/present-go_development_kubernetes

The above command creates a go.mod file.
go.mod tells Go that this folder hosts a set of Go packages that can be found at https://github.com/siuyin/present-go_development_kubernetes .

  > cat go.mod
  module github.com/siuyin/present-go_development_kubernetes
  
  go 1.13



* Write main.go

My convention is to put executable commands in a cmd folder:

  > mkdir -p cmd/hello_app/

cmd/hello_app/main.go:

  package main
  
  import "fmt"
  
  func main() {
          fmt.Println("hello_app")
  }

build and run:

  > go run cmd/hello_app/main.go
  hello_app




* main.go: add http server

cmd/hello_app/main.go:

  ...
  
  func main() {
          fmt.Println("hello_app")
          http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
                  fmt.Fprintf(w, "Hello World!\n")
          })
          log.Fatal(http.ListenAndServe(":8080", nil))
  }

test:

  > go run cmd/hello_app/main.go

In a separate terminal:

  > curl localhost:8080/
  Hello World!



* Time service

Create a folder to hold _our_ time module.

  mkdir time

time/time.go (first attempt)

  package time
  
  import gt "time" // go standard library time package, imported as gt
  
  func Now() string {
  	return gt.Now().Format("15:04:05")
  }

But this poses a testing problem -- it returns a new time each time it is tested!
One way around this issue is to define what is "now" and subtitute a fixed time during testing.



* Time service with testability

.code time/time.go




* Time service test

.code time/time_test.go /10 OMIT/,/20 OMIT/



* Time service example and unit test run

.code time/time_test.go /20 OMIT/,/30 OMIT/

Unit test run:

  > go test time/*.go -v
  === RUN   TestNow
  --- PASS: TestNow (0.00s)
  === RUN   ExampleNow
  --- PASS: ExampleNow (0.00s)
  PASS
  ok      command-line-arguments  0.002s




* Main function

.code cmd/hello_app/main.go /20 OMIT/,/30 OMIT/

webServer and heartBeat are independently executing goroutines.
I prefer this style which I call the "Hakka Roundhouse" monolith.

My preferred way of communication between these internal "processes" is with NATS Streaming or NATS.

Communication with external systems can be through http / json or gRPC.



* webServer

.code cmd/hello_app/main.go /10 OMIT/,/20 OMIT/
.code cmd/hello_app/main.go /30 OMIT/,/40 OMIT/



* heartBeat

.code cmd/hello_app/main.go /10 OMIT/,/20 OMIT/
.code cmd/hello_app/main.go /40 OMIT/,/50 OMIT/

* Functional testing with httptest

cmd/hello_app/main_test.go:

.code cmd/hello_app/main_test.go /10 OMIT/,/20 OMIT/
.code cmd/hello_app/main_test.go /30 OMIT/,/40 OMIT/

* Go modules and external dependencies

"github.com/siuyin/dflt" is an external dependency.

When go test, go run or any other "go" command is run:

- go.mod is updated to include the external dependency

  > cat go.mod
  module github.com/siuyin/present-go_development_kubernetes
  
  go 1.13

  require github.com/siuyin/dflt v0.0.0-20190616123008-ea16caf9b8ef

- go.sum, a checksum file, is created. go.sum should be added to git or your software repository.




* Docker

Download and run docker daemon:

.link https://docs.docker.com/install/

  > docker version
  Client: Docker Engine - Community
   Version:           19.03.5
   API version:       1.40
   Go version:        go1.12.12
   Git commit:        633a0ea838
   Built:             Wed Nov 13 07:29:52 2019
   OS/Arch:           linux/amd64
   Experimental:      false
  
  Server: Docker Engine - Community
   Engine:
    Version:          19.03.5
    API version:      1.40 (minimum version 1.12)
    Go version:       go1.12.12
    Git commit:       633a0ea838
    Built:            Wed Nov 13 07:28:22 2019
    OS/Arch:          linux/amd64
    Experimental:     false
   containerd:
    Version:          1.2.10
    GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
   runc:
    Version:          1.0.0-rc8+dev
    GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
   docker-init:
    Version:          0.18.0
    GitCommit:        fec3683




* Containerizing

Create a .dockerignore:

.code .dockerignore

Create a Dockerfile:

.code Dockerfile



* Dockerfile notes
This is a two stage build:

- Stage 1 uses the Go Alpine image to build. Apline is a smaller image good for pure-go compiles. For cgo (glibc) use the regular golang:1.13 image.
- Stage 2 deploys the result of the build into an empty (scratch) image.  

"go mod download" allows caching of the downloaded modules in the image being built:

- To be effective source code changes reflected in "COPY . ." must be after "go mod download".



* Create and tag an image

The image created below is siuyin/junk with a default tag of "latest".

  docker build . -t siuyin/junk


We can provide a "v1" tag thus:

  docker build . -t siuyin/junk:v1

To list images under siuyin/junk:

  > docker images siuyin/junk
  REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
  siuyin/junk         latest              d3100abc3851        47 minutes ago      7.4MB
  siuyin/junk         v1                  d3100abc3851        47 minutes ago      7.4MB




* Create a container and run it

  docker run -it --rm -p 9090:8080 siuyin/junk 
    or
  docker run -it --rm -p 9090:8080 siuyin/junk:v1

Notes:
-it : i for interactive and t for "allocate TTY (terminal)".
--rm :  to remove the container after it exits.
-p : connect (tcp) port 9090 on host running docker daemon to port 8080 of the code running inside the container.
siuyin/junk : name of the image used create the container. If tag is not specified "latest" is used.

---

On host running docker daemon:

  > curl 127.0.0.1:9090
  Hello, the time is 02:44:07

* kubernetes

Launch development kubernetes 'cluster' with minikube.

check:

  > kubectl get no
  NAME       STATUS   ROLES    AGE    VERSION
  minikube   Ready    master   5d3h   v1.17.0
  
  > kubectl cluster-info
  Kubernetes master is running at https://192.168.39.230:8443
  KubeDNS is running at https://192.168.39.230:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


* kubernetes config

minikube writes its access credentials to a default location: $HOME/.kube/config .

This is where kubectl looks for the config as well.

kubectl can be told which configuration to use with the KUBECONFIG environment variable.

  > export KUBECONFIG=~/go/kube/minikube
  > kubectl get no
  NAME       STATUS   ROLES    AGE    VERSION
  minikube   Ready    master   5d8h   v1.17.0


  > export KUBECONFIG=~/go/kube/rem-config
  > kubectl get no
  NAME            STATUS   ROLES    AGE    VERSION
  remote-kube     Ready    master   603d   v1.15.7


* $HOME/.kube/config:

  apiVersion: v1
  clusters:
  - cluster:
      certificate-authority: /home/siuyin/.minikube/ca.crt
      server: https://192.168.39.230:8443
    name: minikube
  contexts:
  - context:
      cluster: minikube
      user: minikube
    name: minikube
  current-context: minikube
  kind: Config
  preferences: {}
  users:
  - name: minikube
    user:
      client-certificate: /home/siuyin/.minikube/client.crt
      client-key: /home/siuyin/.minikube/client.key

* Create a kubernetes deployment

base/deployment.yaml:

  metadata:
    name: hello-app
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hello-app
    template:
      metadata:
        labels:
          app: hello-app
      spec:
        terminationGracePeriodSeconds: 5
        containers:
        - name: hello-app
          image: siuyin/junk:v1

* Deployment notes

spec/replicas: 1 <-- deploy one replica

spec/selector/matchLabels: app: hello-app <-- manage pods with label app=hello-app

spec/template/metadata/labels: app: hello-app <-- when creating pods give them a label app=hello-app

spec/template/spec/containers[0]/image: siuyin/junk:v1 <-- use this image when creating a pod

---

metadata/name: hello-app <-- name this deployment hello-app
spec/template/spec/containers[0]/name: hello-app <-- name pods created by deployment hello-app


* Deploying hello-app deployment

  > kubectl apply -f base/deployment.yaml
  deployment.apps/hello-app created

  > kubectl get deploy
  NAME                READY   UP-TO-DATE   AVAILABLE   AGE
  hello-app           1/1     1            1           3m38s

  > kubectl get po -l app=hello-app
  NAME                         READY   STATUS    RESTARTS   AGE
  hello-app-7cb499bc66-dxkjm   1/1     Running   0          5m1s

Notes:

deploy is short for deployment
po is short for pod
-l app=hello-app <-- select pods with labels matching app=hello-app

* Deploying NATS Streaming

base/nats-streaming-deployment.yaml:

  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nats-streaming
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nats-streaming
    template:
      metadata:
        labels:
          app: nats-streaming
      spec:
        terminationGracePeriodSeconds: 5
        containers:
        - name: nats-streaming
          image: nats-streaming:0.16.2

* Check NATS Streaming

  > kubectl apply -f base/nats-streaming-deployment.yaml 
  deployment.apps/nats-streaming created

  > kubectl get po -l app=nats-streaming
  NAME                              READY   STATUS    RESTARTS   AGE
  nats-streaming-76599cdffc-hknn9   1/1     Running   0          110s

At this stage, both hello-app and nats-streaming are running in kubernetes.

However neither are  accessible from within or outside the cluster.

We need services.

* hello-app service

base/service.yaml:

.code base/service.yaml

* hello-app service notes:
metadata/name: hello-app <-- this service is named hello-app

spec/selector: app: hello-app <-- this service connects to pods with labels app=hello-app

spec/ports[0]/port: 8080 <-- connect to this service via port 8080

spec/ports[0]/targetPort: 8080 <-- connects to port 8080 on backend pods.

spec/type: NodePort <-- exposes this service on all nodes of the kubernetes cluster

spec/type can also be ClusterIP which is the default type. ClusterIP services are accessible only within the cluster.

Service names (metadata/name) are registered internally with kubernetes DNS and can thus be reached by name.

  eg. from a pod running in kubernetes
  > curl hello-app:8080/

* Accessing hello-app from outside the cluster.

As hello-app service is a NodePort it can be reached from outside the cluter.

  > kubectl cluster-info
  Kubernetes master is running at https://192.168.39.230:8443

  > kubectl get svc | grep hello-app
  hello-app                   NodePort    10.96.30.244    <none>        8080:32449/TCP                15m

  > curl 192.168.39.230:32449/
  Hello, the time is 07:13:09

* Customization with kustomize

We now have fully working deployments for hello-app and nats-streaming.

However we often want to deploy multiple instances of our applications within a kubernetes cluster.
Say my-hello-app and my-nats-streaming.

kustomize allows us to easily do this.

kustomize is a single static binary download.

 > kustomize version
 {Version:kustomize/v3.4.0 GitCommit:2c9635967a2b1469d605a91a1d040bd27c73ca7d BuildDate:2019-11-12T05:00:57Z GoOs:linux GoArch:amd64}

* base/kustomization.yaml:

.code base/kustomization.yaml

view kustomization:

  > kustomize build base


with kubectl > 1.14:

  > kubectl kustomize base

* kustomized hello-app service

  > kustomize build base
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      stage: dev
      sys: my
    name: my-hello-app
  spec:
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: hello-app
      stage: dev
      sys: my
    type: NodePort

Note: two new labels: sys=my and stage=dev and name-prefix="my-"

* Deploying and deleting

  > kustomize build base|kubectl apply -f -
  configmap/my-cfg-f7974mhb9k created
  service/my-hello-app created
  service/my-nats-streaming created
  deployment.apps/my-hello-app created
  deployment.apps/my-nats-streaming created

  > kustomize build base|kubectl delete -f -
  configmap "my-cfg-f7974mhb9k" deleted
  service "my-hello-app" deleted
  service "my-nats-streaming" deleted
  deployment.apps "my-hello-app" deleted
  deployment.apps "my-nats-streaming" deleted

* Selecting on the new labels

  > kubectl get cm,svc,deploy,po -l sys=my,stage=dev
  NAME                          DATA   AGE
  configmap/my-cfg-f7974mhb9k   1      35s
  
  NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
  service/my-hello-app        NodePort    10.96.186.251   <none>        8080:31491/TCP   35s
  service/my-nats-streaming   ClusterIP   10.96.121.165   <none>        4222/TCP         35s
  
  NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
  deployment.apps/my-hello-app        1/1     1            1           35s
  deployment.apps/my-nats-streaming   1/1     1            1           34s
  
  NAME                                    READY   STATUS    RESTARTS   AGE
  pod/my-hello-app-57fbdbf5d4-4q5x8       1/1     Running   0          34s
  pod/my-nats-streaming-d5bb4488f-lq4zz   1/1     Running   0          34s

* Creating a production overlay

overlays/prod/kustomization.yaml:

.code overlays/prod/kustomization.yaml

* kustomized hello-app prod service

  > kustomize build overlays/prod 
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      stage: prod
      sys: my
    name: my-hello-app-prod
  spec:
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: hello-app
      stage: prod
      sys: my
    type: NodePort

Note: changed label stage=prod and names have suffix "-prod"

* Deploying and deleting the "prod" services

  > kustomize build overlays/prod|kubectl apply -f -
  configmap/my-cfg-prod-4tbf8mh6f8 created
  service/my-hello-app-prod created
  service/my-nats-streaming-prod created
  deployment.apps/my-hello-app-prod created
  deployment.apps/my-nats-streaming-prod created

  > kustomize build overlays/prod|kubectl delete -f -
  configmap "my-cfg-prod-4tbf8mh6f8" deleted
  service "my-hello-app-prod" deleted
  service "my-nats-streaming-prod" deleted
  deployment.apps "my-hello-app-prod" deleted
  deployment.apps "my-nats-streaming-prod" deleted

* Continuous development of hello-app

To develop and deploy hello-app:

- Edit go source file: go build cmd/hello-app/main.go
- Build and tag docker image: docker build . -t siuyin/junk:v32
- Edit base/deployment.yaml to reflect new siuyin/junk:v32 image tag
- Deploy to kubernetes: kustomize build base | kubectl apply -f -

We can automate tag creation and updating of kustomize files with *skaffold*.

  > skaffold version
  v1.2.0


* skaffold.yaml

.code skaffold.yaml

Note the absence an image tag for image: siuyin/junk .

* skaffold render

  > skaffold render
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      skaffold.dev/deployer: kustomize
      stage: dev
      sys: my
    name: my-hello-app
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hello-app
        stage: dev
        sys: my
    template:
      ...
      spec:
        containers:
        - image: siuyin/junk:28dab121d7d329d7fd2d05e4829f2bbb64eee06be5cf25a679c4f55009b665fc
          name: hello-app

Note the generated tag.

* skaffold

- builds artifact: docker image siuyin/junk
- tags the image with the SHA1 sum of the image
- pushes the image to a docker registry (if deploying to a remote cluster)
- invokes kustomize passing the correct image tag
- invokes kustomize/kubectl to deploy to kubernetes

* skaffold run

  > skaffold run --tail
  ...
  Successfully built 28dab121d7d3
  Successfully tagged siuyin/junk:494f650-dirty
  Tags used in deployment:
   - siuyin/junk -> siuyin/junk:28dab121d7d329d7fd2d05e4829f2bbb64eee06be5cf25a679c4f55009b665fc
     local images can't be referenced by digest. They are tagged and referenced by a unique ID instead
  Starting deploy...
   - configmap/my-cfg-f7974mhb9k created
   - service/my-hello-app created
   - service/my-nats-streaming created
   - deployment.apps/my-hello-app created
   - deployment.apps/my-nats-streaming created
  [my-hello-app-97f8747fd-2kgdl hello-app] hello_app
  [my-hello-app-97f8747fd-2kgdl hello-app] 09:14:39

The --tail option streams logs to the invoking terminal.


* Metrics

  > kubectl top node
  NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
  minikube   1069m        53%    1349Mi          72%

  > kubectl top pods
  NAME                                 CPU(cores)   MEMORY(bytes)   
  my-hello-app-97f8747fd-2kgdl         0m           1Mi             
  my-nats-streaming-667c98485b-d86s6   1m           3Mi

kubectl top pods can be used to estimate cpu (milli-cores) and memory requirements.

* Resource quotas

In production, apply resource quota like thus:

  apiVersion: v1
  kind: Pod
  metadata:
    name: quota-mem-cpu-demo
  spec:
    containers:
    - name: quota-mem-cpu-demo-ctr
      image: nginx
      resources:
        limits:
          memory: "800Mi"
          cpu: "800m" 
        requests:
          memory: "600Mi"
          cpu: "400m"

* Retrieving pod logs

  > kubectl logs pod/my-hello-app-97f8747fd-2kgdl
  hello_app
  09:14:39
  09:14:44
  09:14:49

* End-to-end testing

test/end2end/main.go:

.code test/end2end/main.go /20 OMIT/,/30 OMIT/

* End-to-end testing (2)

.code test/end2end/main.go /30 OMIT/,/40 OMIT/
      
  > ENDPOINT=http://192.168.39.230:31377/  go run test/end2end/main.go 
  End to end testing.
  
  getting http://192.168.39.230:31377/
  checking status code
  checking response
  
  tests complete. Took 0.005287454 seconds

* End-to-end testing with testing library

test/end2end/main_test.go:

.code test/end2end/main_test.go /10 OMIT/,/20 OMIT/
.code test/end2end/main_test.go /30 OMIT/,/40 OMIT/

In the above, each t.Run is a sub-test run independently.
Two sub-tests are run -- to get "/" and "/a".

get is a test helper function that actually executes the test.

* get test helper

.code test/end2end/main_test.go /40 OMIT/,/50 OMIT/

* End-to-end test run

  > ENDPOINT=http://192.168.39.230:31377 go test -v
  === RUN   TestEndToEnd
  === RUN   TestEndToEnd/root
  === RUN   TestEndToEnd//a
  --- PASS: TestEndToEnd (0.01s)
      --- PASS: TestEndToEnd/root (0.00s)
          main_test.go:18: connecting to endpoint: http://192.168.39.230:31377/
          main_test.go:18: status code: 200
          main_test.go:18: body contents OK
      --- PASS: TestEndToEnd//a (0.00s)
          main_test.go:19: connecting to endpoint: http://192.168.39.230:31377/a
          main_test.go:19: status code: 200
          main_test.go:19: body contents OK
  PASS
  ok      github.com/siuyin/present-go_development_kubernetes/test/end2end        0.016s

* End-to-end test failure

  > ENDPOINT=http://192.168.39.230:1234 go test
  === RUN   TestEndToEnd
  === RUN   TestEndToEnd/root
  === RUN   TestEndToEnd//a
  --- FAIL: TestEndToEnd (0.00s)
      --- FAIL: TestEndToEnd/root (0.00s)
          main_test.go:18: connecting to endpoint: http://192.168.39.230:1234/
          main_test.go:18: could not reach endpoint http://192.168.39.230:1234/: Get http://192.168.39.230:1234/: dial tcp 192.168.39.230:1234: connect: connection refused
      --- FAIL: TestEndToEnd//a (0.00s)
          main_test.go:19: connecting to endpoint: http://192.168.39.230:1234/a
          main_test.go:19: could not reach endpoint http://192.168.39.230:1234/a: Get http://192.168.39.230:1234/a: dial tcp 192.168.39.230:1234: connect: connection refused
  FAIL
  exit status 1
  FAIL    github.com/siuyin/present-go_development_kubernetes/test/end2end        0.015s


* Presentation and code download


.link https://github.com/siuyin/present-go_development_kubernetes

